{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd src/metavoice-src-main\n",
    "\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import librosa\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from fam.llm.adapters import FlattenedInterleavedEncodec2Codebook\n",
    "from fam.llm.decoders import EncodecDecoder\n",
    "from fam.llm.fast_inference_utils import build_model, main\n",
    "from fam.llm.inference import (\n",
    "    EncodecDecoder,\n",
    "    InferenceConfig,\n",
    "    Model,\n",
    "    TiltedEncodec,\n",
    "    TrainedBPETokeniser,\n",
    "    get_cached_embedding,\n",
    "    get_cached_file,    \n",
    "    get_enhancer,\n",
    ")\n",
    "from fam.llm.utils import (\n",
    "    check_audio_file,\n",
    "    get_default_dtype,\n",
    "    get_device,\n",
    "    normalize_text,\n",
    ")\n",
    "\n",
    "model_name = \"metavoiceio/metavoice-1B-v0.1\"\n",
    "seed = 1337\n",
    "output_dir = \"outputs\"\n",
    "_dtype = get_default_dtype()\n",
    "_device = 'cuda:0'\n",
    "_model_dir = snapshot_download(repo_id=model_name)\n",
    "first_stage_adapter = FlattenedInterleavedEncodec2Codebook(end_of_audio_token=1024)\n",
    "output_dir = output_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "second_stage_ckpt_path = f\"{_model_dir}/second_stage.pt\"\n",
    "config_second_stage = InferenceConfig(\n",
    "    ckpt_path=second_stage_ckpt_path,\n",
    "    num_samples=1,\n",
    "    seed=seed,\n",
    "    device=_device,\n",
    "    dtype=_dtype,\n",
    "    compile=False,\n",
    "    init_from=\"resume\",\n",
    "    output_dir=output_dir,\n",
    ")\n",
    "data_adapter_second_stage = TiltedEncodec(end_of_audio_token=1024)\n",
    "llm_second_stage = Model(\n",
    "    config_second_stage, TrainedBPETokeniser, EncodecDecoder, data_adapter_fn=data_adapter_second_stage.decode\n",
    ")\n",
    "enhancer = get_enhancer(\"df\")\n",
    "\n",
    "precision = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16}[_dtype]\n",
    "model, tokenizer, smodel, model_size = build_model(\n",
    "    precision=precision,\n",
    "    checkpoint_path=Path(f\"{_model_dir}/first_stage.pt\"),\n",
    "    spk_emb_ckpt_path=Path(f\"{_model_dir}/speaker_encoder.pt\"),\n",
    "    device=_device,\n",
    "    compile=True,\n",
    "    compile_prefill=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain emotion direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: provide your own audio files as (neutral, empathetic) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put in your own audio files as (neutral, empathetic) pairs\n",
    "audio_pairs = [\n",
    "    ('/proj/afosr/metavoice/misc_audio_files/neutral_oprah.wav', '/proj/afosr/metavoice/misc_audio_files/oprah_empathetic_concatenated.wav'),\n",
    "    ('/proj/afosr/metavoice/misc_audio_files/neutral_vt2NjqXKzyA.wav', '/proj/afosr/metavoice/misc_audio_files/vt2NjqXKzyA_empathetic_concatenated.wav')\n",
    "]\n",
    "\n",
    "source_speaker_audio_path = #plug in audio file for voice cloning\n",
    "source_emb = get_cached_embedding(source_speaker_audio_path, smodel).to(device=_device, dtype=precision)\n",
    "\n",
    "speaker_pair_embs = [\n",
    "    (get_cached_embedding(neutral_audio_path, smodel).to(device=_device, dtype=precision), \n",
    "    get_cached_embedding(emotional_audio_path, smodel).to(device=_device, dtype=precision)) for neutral_audio_path, emotional_audio_path in audio_pairs\n",
    "]\n",
    "\n",
    "emo_dirs = [emotional_emb - neutral_emb for neutral_emb, emotional_emb in speaker_pair_embs]\n",
    "emo_dirs = [emo_dir / torch.linalg.norm(emo_dir, dim=-1, keepdim=True) for emo_dir in emo_dirs]\n",
    "\n",
    "emo_dir = sum(emo_dirs) / len(emo_dirs)\n",
    "emo_dir = emo_dir / torch.linalg.norm(emo_dir, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: use our pre-computed emotion directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/afosr/metavoice/final_mv_env/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/proj/afosr/metavoice/emoknob/src/metavoice-src-main\n",
      "available emotions: dict_keys(['charisma', 'empathetic', 'angry', 'contempt', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'desire', 'doubt', 'empathic pain', 'envy', 'joy', 'neutral', 'romance', 'sarcasm', 'tiredness', 'triump'])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "emo_dirs = pickle.load(open('../all_emo_dirs.pkl', 'rb'))\n",
    "print(f\"available emotions: {emo_dirs.keys()}\")\n",
    "emo_dir = emo_dirs['happy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strength = 0.3 #set strength of emotion control\n",
    "edited_emb = source_emb + strength * torch.tensor(emo_dir, device=_device, dtype=precision)\n",
    "\n",
    "\n",
    "top_p=0.95\n",
    "guidance_scale=3.0#3.0\n",
    "temperature=1.0\n",
    "text = normalize_text(text)\n",
    "\n",
    "start = time.time()\n",
    "# first stage LLM\n",
    "tokens = main(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_size=model_size,\n",
    "    prompt=text,\n",
    "    spk_emb=edited_emb,\n",
    "    top_p=torch.tensor(top_p, device=_device, dtype=precision),\n",
    "    guidance_scale=torch.tensor(guidance_scale, device=_device, dtype=precision),\n",
    "    temperature=torch.tensor(temperature, device=_device, dtype=precision),\n",
    ")\n",
    "text_ids, extracted_audio_ids = first_stage_adapter.decode([tokens])\n",
    "\n",
    "b_speaker_embs = edited_emb.unsqueeze(0)\n",
    "\n",
    "# second stage LLM + multi-band diffusion model\n",
    "wav_files = llm_second_stage(\n",
    "    texts=[text],\n",
    "    encodec_tokens=[torch.tensor(extracted_audio_ids, dtype=torch.int32, device=_device).unsqueeze(0)],\n",
    "    speaker_embs=b_speaker_embs,\n",
    "    batch_size=1,\n",
    "    guidance_scale=None,\n",
    "    top_p=None,\n",
    "    top_k=200,\n",
    "    temperature=1.0,\n",
    "    max_new_tokens=None,\n",
    ")\n",
    "\n",
    "wav_file = wav_files[0]\n",
    "with tempfile.NamedTemporaryFile(suffix=\".wav\") as enhanced_tmp:\n",
    "    enhancer(str(wav_file) + \".wav\", enhanced_tmp.name)\n",
    "    shutil.copy2(enhanced_tmp.name, str(wav_file) + \".wav\")\n",
    "    print(f\"\\nSaved audio to {wav_file}.wav\")\n",
    "\n",
    "output_path = str(wav_file) + \".wav\"\n",
    "\n",
    "\n",
    "# Display the generated audio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "display(Audio(output_path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_mv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
